from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType
import time
import json

spark = SparkSession \
  .builder \
  .appName("streaming") \
  .master("local[*]") \
  .getOrCreate()

spark

schema = StructType([
    StructField("latitude", DoubleType(), True),
    StructField("longitude", DoubleType(), True),
    StructField("date", StringType(), True),
    StructField("customer_id", IntegerType(), True),
    StructField("employee_id", IntegerType(), True),
    StructField("quantity_products", IntegerType(), True),
    StructField("order_id", StringType(), True)
])

streaming_df = spark.readStream.format("socket").option("host", "localhost").option("port", "8000").load()
json_df = streaming_df.select(from_json(streaming_df.value, schema).alias("data")).selectExpr("data.*")
writing_df = json_df.writeStream.format("console").outputMode("update").start()
writing_df.awaitTermination()